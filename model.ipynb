{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "411e9943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camer\\Desktop\\Code\\CustomerChurnClassifier\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "c:\\Users\\camer\\Desktop\\Code\\CustomerChurnClassifier\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "import lightgbm as lgbm\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.discovery import all_estimators\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, BaggingClassifier\n",
    "\n",
    "RANDOM_STATE = 123\n",
    "TRAIN_SIZE = 0.8\n",
    "ALL_ESTIMATORS_RUN = False\n",
    "TRIAL_SIZE = 50\n",
    "RETUNE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3ea122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(\"data/customer_churn_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ad14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow same steps as EDA for data cleaning and type conversion\n",
    "df[\"customer_id\"] = df[\"customer_id\"].astype(\"int64\")\n",
    "df[\"tenure\"] = df[\"tenure\"].astype(\"int64\")\n",
    "df[\"monthly_charges\"] = df[\"monthly_charges\"].astype(\"float64\")\n",
    "df[\"total_charges\"] = pd.to_numeric(df[\"total_charges\"], errors='coerce')\n",
    "df[\"contract\"] = df[\"contract\"].astype(\"category\")\n",
    "df[\"payment_method\"] = df[\"payment_method\"].astype(\"category\")\n",
    "# almost 10% of the dataset has missing values for internet_service, so better to fill than to drop\n",
    "df[\"internet_service\"] = df[\"internet_service\"].fillna(\"Unknown Service\")\n",
    "df[\"internet_service\"] = df[\"internet_service\"].astype(\"category\")\n",
    "df[\"tech_support\"] = df[\"tech_support\"].apply(lambda x: True if x == 'Yes' else False).astype(\"bool\")\n",
    "df[\"online_security\"] = df[\"online_security\"].apply(lambda x: True if x == 'Yes' else False).astype(\"bool\")\n",
    "df[\"support_calls\"] = df[\"support_calls\"].astype(\"int64\")\n",
    "df[\"churn\"] = df[\"churn\"].apply(lambda x: True if x == 'Yes' else False).astype(\"bool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b216b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical features\n",
    "df = pd.get_dummies(df, columns=[\"contract\", \"payment_method\", \"internet_service\"], drop_first=True)\n",
    "\n",
    "# split dataset\n",
    "independent = df.drop(columns=[\"customer_id\", \"churn\"])\n",
    "dependent = df[\"churn\"]\n",
    "independent_train, independent_test, dependent_train, dependent_test = train_test_split(independent, dependent, test_size=1-TRAIN_SIZE, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88fabc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scikit-learn all estimators to get a baseline of top models to train\n",
    "if ALL_ESTIMATORS_RUN:\n",
    "    models_list = []\n",
    "    models_df = pd.DataFrame(columns=[\"Model\", \"Accuracy\"])\n",
    "    estimators = all_estimators(type_filter='classifier')\n",
    "    for name, ClassifierClass in estimators:\n",
    "        try:\n",
    "            model = ClassifierClass()\n",
    "            model.fit(independent_train, dependent_train)\n",
    "            score = model.score(independent_test, dependent_test)\n",
    "            models_list.append((name, score))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    models_df = pd.DataFrame(models_list, columns=[\"Model\", \"Accuracy\"])\n",
    "    models_df.sort_values(by=\"Accuracy\", ascending=False, inplace=True)\n",
    "    models_df.to_csv(\"model_baselines.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd47ec",
   "metadata": {},
   "source": [
    "- Trees seem to have the best accuracy for this dataset, with HistGradientBoostingClassifier being the best overall model (with default hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "198c5428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camer\\Desktop\\Code\\CustomerChurnClassifier\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [11:35:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.83975\n",
      "CatBoost Accuracy: 0.84925\n",
      "LightGBM Accuracy: 0.8485\n"
     ]
    }
   ],
   "source": [
    "# try a few other models (XGBoost, CatBoost, LightGBM)\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(independent_train, dependent_train)\n",
    "xgb_score = xgb_model.score(independent_test, dependent_test)\n",
    "print(f\"XGBoost Accuracy: {xgb_score}\")\n",
    "\n",
    "# CatBoost\n",
    "catboost_model = catboost.CatBoostClassifier(random_state=RANDOM_STATE, verbose=0)\n",
    "catboost_model.fit(independent_train, dependent_train)\n",
    "catboost_score = catboost_model.score(independent_test, dependent_test)\n",
    "print(f\"CatBoost Accuracy: {catboost_score}\")\n",
    "\n",
    "# LightGBM\n",
    "lgbm_model = lgbm.LGBMClassifier(random_state=RANDOM_STATE, verbose=-1)\n",
    "lgbm_model.fit(independent_train, dependent_train)\n",
    "lgbm_score = lgbm_model.score(independent_test, dependent_test)\n",
    "print(f\"LightGBM Accuracy: {lgbm_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22cdeb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camer\\Desktop\\Code\\CustomerChurnClassifier\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Neural Network Accuracy: 0.7057499885559082\n"
     ]
    }
   ],
   "source": [
    "# Try TensorFlow Neural Network\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "tf_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(independent_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "tf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "tf_model.fit(independent_train, dependent_train, epochs=50, batch_size=32, verbose=0)\n",
    "tf_loss, tf_accuracy = tf_model.evaluate(independent_test, dependent_test, verbose=0)\n",
    "print(f\"TensorFlow Neural Network Accuracy: {tf_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d6820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Will focus on top 5 from scikit-learn:\n",
    "(HistGradientBoostingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, BaggingClassifier), \n",
    "and also XGBoost, CatBoost, LightGBM, and TensorFlow models for hyperparameter tuning using Optuna.\n",
    "\"\"\"\n",
    "\n",
    "# start with HistGradientBoostingClassifier \n",
    "def objective_HistGradientBoosting(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 1.0),\n",
    "        'max_iter': trial.suggest_int('max_iter', 50, 5000),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 50),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 200),\n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "    model = HistGradientBoostingClassifier(**params)\n",
    "    model.fit(independent_train, dependent_train)\n",
    "    return model.score(independent_test, dependent_test)\n",
    "\n",
    "if RETUNE:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective_HistGradientBoosting, n_trials=TRIAL_SIZE)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Accuracy: {trial.value}\")\n",
    "    print(\"Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key}: {value}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c22a5b",
   "metadata": {},
   "source": [
    "- Reached 84.925% accuracy with HistGradientBoostingClassifier after hyperparameter tuning with params:\n",
    "\n",
    "- Params: \n",
    "    - learning_rate: 0.07332409159255568\n",
    "    - max_iter: 4872\n",
    "    - max_leaf_nodes: 827\n",
    "    - max_depth: 2\n",
    "    - min_samples_leaf: 153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc31d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same with AdaBoostClassifier\n",
    "\n",
    "def objective_AdaBoost(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 5000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 1.0),\n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "    model = AdaBoostClassifier(**params)\n",
    "    model.fit(independent_train, dependent_train)\n",
    "    return model.score(independent_test, dependent_test)\n",
    "\n",
    "if RETUNE:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective_AdaBoost, n_trials=TRIAL_SIZE)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Accuracy: {trial.value}\")\n",
    "    print(\"Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1bd6d",
   "metadata": {},
   "source": [
    "- 84.9% accuracy with AdaBoostClassifier after hyperparameter tuning\n",
    "\n",
    "- Params:\n",
    "    - n_estimators: 4593\n",
    "    - learning_rate: 0.007281147204159136\n",
    "    - algorithm: 'SAMME.R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8902a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier\n",
    "\n",
    "def objective_GradientBoosting(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 5000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 1.0),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 100),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 200),\n",
    "        'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.5),\n",
    "        'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 1.0),\n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "    model = GradientBoostingClassifier(**params)\n",
    "    model.fit(independent_train, dependent_train)\n",
    "    return model.score(independent_test, dependent_test)\n",
    "\n",
    "if RETUNE:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective_GradientBoosting, n_trials=TRIAL_SIZE)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Accuracy: {trial.value}\")\n",
    "    print(\"Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f1497",
   "metadata": {},
   "source": [
    "- 84.825% accuracy with RandomForestClassifier after hyperparameter tuning\n",
    "- Params:\n",
    "    - n_estimators: 1083\n",
    "    - learning_rate: 0.24125663623896226\n",
    "    - min_samples_split: 159\n",
    "    - max_depth: 20\n",
    "    - min_samples_leaf: 126\n",
    "    - min_weight_fraction_leaf: 0.06575178557966645\n",
    "    - min_impurity_decrease: 0.1742699284704499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9277443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "def objective_RandomForest(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 5000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 100),\n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(independent_train, dependent_train)\n",
    "    return model.score(independent_test, dependent_test)\n",
    "\n",
    "if RETUNE:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective_RandomForest, n_trials=TRIAL_SIZE)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Accuracy: {trial.value}\")\n",
    "    print(\"Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79da25a",
   "metadata": {},
   "source": [
    "- 84.9% accuracy with RandomForestClassifier after hyperparameter tuning\n",
    "- Params: \n",
    "    - n_estimators: 3483\n",
    "    - max_depth: 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db0b663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaggingClassifier\n",
    "def objective_Bagging(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 5000),\n",
    "        'max_samples': trial.suggest_float('max_samples', 0.1, 1.0),\n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "    model = BaggingClassifier(**params)\n",
    "    model.fit(independent_train, dependent_train)\n",
    "    return model.score(independent_test, dependent_test)\n",
    "\n",
    "if RETUNE:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective_Bagging, n_trials=TRIAL_SIZE)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Accuracy: {trial.value}\")\n",
    "    print(\"Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec8aec",
   "metadata": {},
   "source": [
    "- 84.9% accuracy with BaggingClassifier after hyperparameter tuning\n",
    "- Params:\n",
    "    - n_estimators: 3146\n",
    "    - max_samples: 0.18863457093513236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1127b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "def objective_XGBoost(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 5000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 100),\n",
    "        'min_split_loss': trial.suggest_float('min_split_loss', 0.0, 100.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 200),\n",
    "        'max_delta_step': trial.suggest_int('max_delta_step', 0, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.0, 1.0),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(independent_train, dependent_train)\n",
    "    return model.score(independent_test, dependent_test)\n",
    "\n",
    "if RETUNE:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective_XGBoost, n_trials=TRIAL_SIZE)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Accuracy: {trial.value}\")\n",
    "    print(\"Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e66ad6",
   "metadata": {},
   "source": [
    "- 84.975% accuracy with XGBClassifier after hyperparameter tuning\n",
    "- Params:\n",
    "    - n_estimators: 3904\n",
    "    - learning_rate: 0.5293985800479314\n",
    "    - max_depth: 65\n",
    "    - min_split_loss: 18.78518998907755\n",
    "    - min_child_weight: 1\n",
    "    - max_delta_step: 40\n",
    "    - subsample: 0.3444699062771349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "091c54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "def objective_CatBoost(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 1.0),\n",
    "        'depth': trial.suggest_int('depth', 1, 16),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    model = catboost.CatBoostClassifier(**params)\n",
    "    model.fit(independent_train, dependent_train)\n",
    "    return model.score(independent_test, dependent_test)\n",
    "\n",
    "if RETUNE:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective_CatBoost, n_trials=TRIAL_SIZE)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Accuracy: {trial.value}\")\n",
    "    print(\"Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9bb61",
   "metadata": {},
   "source": [
    "- 84.925% accuracy with CatBoostClassifier after hyperparameter tuning\n",
    "- Params:\n",
    "    - n_estimators: 11\n",
    "    - learning_rate: 0.8307552618765016\n",
    "    - depth: 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e94ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "def objective_LightGBM(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 1.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 100),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 200),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    model = lgbm.LGBMClassifier(**params)\n",
    "    model.fit(independent_train, dependent_train)\n",
    "    return model.score(independent_test, dependent_test)\n",
    "\n",
    "if RETUNE:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective_LightGBM, n_trials=TRIAL_SIZE)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Accuracy: {trial.value}\")\n",
    "    print(\"Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37660a6f",
   "metadata": {},
   "source": [
    "- 84.925% accuracy with LGBMClassifier after hyperparameter tuning\n",
    "- Params:\n",
    "    - n_estimators: 545\n",
    "    - learning_rate: 0.008694046219517741\n",
    "    - num_leaves: 209\n",
    "    - max_depth: 8\n",
    "    - min_data_in_leaf: 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2d2aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Neural Network\n",
    "def objective_TensorFlow(trial):\n",
    "    tf.random.set_seed(RANDOM_STATE)\n",
    "    model = tf.keras.Sequential([tf.keras.Input(shape=(independent_train.shape[1],))])\n",
    "    curr_layer_units = trial.suggest_int('units_layer_2', 16, 1024)\n",
    "    for i in range(trial.suggest_int('num_hidden_layers', 1, 5)):\n",
    "        model.add(tf.keras.layers.Dropout(trial.suggest_float(f'dropout_rate_{i+2}', 0.0, 0.5)))\n",
    "        curr_layer_units = trial.suggest_int(f'units_layer_{i+2}', 2, curr_layer_units)\n",
    "        model.add(tf.keras.layers.Dense(curr_layer_units, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(independent_train, dependent_train, epochs=trial.suggest_int('epochs', 10, 250), batch_size=trial.suggest_int('batch_size', 16, 128), verbose=0)\n",
    "    loss, accuracy = model.evaluate(independent_test, dependent_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "if RETUNE:\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective_TensorFlow, n_trials=TRIAL_SIZE)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"Accuracy: {trial.value}\")\n",
    "    print(\"Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf666b7",
   "metadata": {},
   "source": [
    "83.6% accuracy with TensorFlow Neural Network after hyperparameter tuning\n",
    "- Params:\n",
    "    - units_layer_2: 397\n",
    "    - num_hidden_layers: 2\n",
    "    - dropout_rate_2: 0.029077278360171818\n",
    "    - dropout_rate_3: 0.002936539820047132\n",
    "    - units_layer_3: 173\n",
    "    - epochs: 235\n",
    "    - batch_size: 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a20ddca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camer\\Desktop\\Code\\CustomerChurnClassifier\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [11:41:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final XGBoost Model Accuracy: 0.84975\n",
      "\n",
      "Sample Predictions:\n",
      "Index: 4025, Prediction: True Actual: True\n",
      "Index: 721, Prediction: True Actual: True\n",
      "Index: 12930, Prediction: False Actual: False\n",
      "Index: 13259, Prediction: False Actual: False\n",
      "Index: 10910, Prediction: False Actual: False\n",
      "Index: 4332, Prediction: True Actual: False\n",
      "Index: 784, Prediction: False Actual: False\n",
      "Index: 19395, Prediction: True Actual: True\n",
      "Index: 16573, Prediction: False Actual: False\n",
      "Index: 1878, Prediction: False Actual: True\n",
      "Accuracy on 10 random samples: 80%\n"
     ]
    }
   ],
   "source": [
    "# best accuracy was with XGBoost model a1t 84.975% accuracy, so recreate that model\n",
    "\n",
    "final_model = xgb.XGBClassifier(\n",
    "    n_estimators=3904,\n",
    "    learning_rate=0.5293985800479314,\n",
    "    max_depth=65,\n",
    "    min_split_loss=18.78518998907755,\n",
    "    min_child_weight=1,\n",
    "    max_delta_step=40,\n",
    "    subsample=0.3444699062771349,\n",
    "    random_state=RANDOM_STATE,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "final_model.fit(independent_train, dependent_train)\n",
    "final_score = final_model.score(independent_test, dependent_test)\n",
    "print(f\"Final XGBoost Model Accuracy: {final_score}\\n\")\n",
    "\n",
    "# select 10 random samples and predict churn\n",
    "sample_indices = np.random.choice(independent_test.index, size=10, replace=False)\n",
    "sample_data = independent_test.loc[sample_indices]\n",
    "sample_predictions = final_model.predict(sample_data)\n",
    "print(\"Sample Predictions:\")\n",
    "correct_count = 0\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    if bool(sample_predictions[i]) == dependent_test.loc[idx]:\n",
    "        correct_count += 1\n",
    "    print(f\"Index: {idx}, Prediction: {bool(sample_predictions[i])} Actual: {dependent_test.loc[idx]}\")\n",
    "\n",
    "print(f\"Accuracy on 10 random samples: {correct_count * 10}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43b7002",
   "metadata": {},
   "source": [
    "# Further Research\n",
    "\n",
    "- Accuracy can further be improved with a wider search for optuna hyperparameter tuning\n",
    "- Ensembling multiple models may also increase accuracy\n",
    "- Feature engineering is likely to improve performance\n",
    "- Normalizing the data could also lead to improvements\n",
    "- The one that is most likely to improve given more time is the TensorFlow model, I wasn't able to do a very exhaustive search or spend much time tuning it due to hardware limitations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
